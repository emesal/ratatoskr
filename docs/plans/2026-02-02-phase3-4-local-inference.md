# Ratatoskr Phase 3-4: Local Inference & Token Counting

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Status:** ğŸŸ¡ IN PROGRESS (11/15 tasks complete)

**Goal:** Add local inference capabilities (embeddings via fastembed-rs, NLI via ONNX) and token counting, enabling Ã¶rlÃ¶g to run high-volume operations without API costs.

**Architecture:** Hybrid approach â€” `fastembed-rs` for batteries-included embeddings, raw `ort` for NLI cross-encoder models. Token counting via HuggingFace `tokenizers` crate with extensible provider pattern.

**Tech Stack:** Rust, fastembed, ort, tokenizers, hf-hub

**Progress:**
- âœ… Tasks 1-6: Core infrastructure (dependencies, tokenizer, types, device, fastembed)
- âœ… Tasks 7-9: ONNX NLI, model manager, capabilities
- âœ… Tasks 10-11: Builder integration, gateway updates
- â³ Task 12: Generate impl (via llm crate)
- â³ Tasks 13-15: Live tests, lint, documentation

**Latest Commit:** `0a17cff` - feat(local-inference): add phase 3 foundation (tasks 1-6)

---

## 1. Architecture Overview

This phase adds local inference capabilities via two new provider types:

```
                          ModelGateway trait
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                       â–¼                       â–¼
   EmbeddedGateway         (future)              (future)
         â”‚                ServiceClient        RemoteOnnxProvider
         â”‚
         â”œâ”€â”€ llm crate (chat providers)
         â”‚     â””â”€â”€ OpenRouter, Anthropic, OpenAI, Ollama, Google
         â”‚
         â”œâ”€â”€ HuggingFaceClient (API-based)
         â”‚     â””â”€â”€ embed, NLI, classify via HTTP
         â”‚
         â”œâ”€â”€ FastEmbedProvider (new)
         â”‚     â””â”€â”€ local embeddings via fastembed-rs
         â”‚
         â”œâ”€â”€ OnnxNliProvider (new)
         â”‚     â””â”€â”€ local NLI via ort + cross-encoder models
         â”‚
         â””â”€â”€ TokenizerRegistry (new)
               â””â”€â”€ token counting via tokenizers crate
```

The router gains new provider options:

```toml
[routing]
embed = "fastembed"      # local via fastembed-rs
nli = "onnx"             # local via ort
classify = "huggingface" # still API-based
chat = "openrouter"      # via llm crate
```

---

## 2. New Dependencies

```toml
[features]
# existing
huggingface = ["dep:reqwest"]

# new
local-inference = ["dep:fastembed", "dep:ort", "dep:tokenizers", "dep:hf-hub"]
cuda = ["ort/cuda"]  # optional GPU support

[dependencies]
# Local inference (optional)
fastembed = { version = "4", optional = true }
ort = { version = "2", optional = true, default-features = false }
tokenizers = { version = "0.20", optional = true, default-features = false, features = ["http"] }
hf-hub = { version = "0.3", optional = true }

# For blocking ops in async context
tokio = { version = "1", features = ["rt", "sync"] }
```

Key points:
- All local inference behind `local-inference` feature flag
- `cuda` is separate feature for GPU support
- `tokenizers` with `http` feature fetches from HuggingFace hub
- `hf-hub` handles model downloads with caching

---

## 3. Token Counting System

```rust
// src/tokenizer/mod.rs

/// Trait for tokenizer implementations, allowing future expansion
pub trait TokenizerProvider: Send + Sync {
    fn count_tokens(&self, text: &str) -> Result<usize>;
    fn tokenize(&self, text: &str) -> Result<Vec<u32>>;
}

/// HuggingFace tokenizers implementation
pub struct HfTokenizer {
    inner: tokenizers::Tokenizer,
}

impl HfTokenizer {
    /// Load from HuggingFace hub
    pub fn from_hub(repo_id: &str) -> Result<Self>;

    /// Load from local file
    pub fn from_file(path: &Path) -> Result<Self>;
}

/// Registry mapping model names to tokenizers
pub struct TokenizerRegistry {
    tokenizers: RwLock<HashMap<String, Arc<dyn TokenizerProvider>>>,
    model_mappings: HashMap<String, TokenizerSource>,
}

pub enum TokenizerSource {
    HuggingFace { repo_id: String },
    Local { path: PathBuf },
    Alias { target: String },  // "claude-sonnet-4" â†’ "claude"
}

impl TokenizerRegistry {
    pub fn new() -> Self;

    /// Register a model â†’ tokenizer mapping
    pub fn register(&mut self, model_pattern: &str, source: TokenizerSource);

    /// Count tokens, loading tokenizer lazily if needed
    pub fn count_tokens(&self, text: &str, model: &str) -> Result<usize>;
}
```

Default mappings (built-in):

| Model Pattern | Tokenizer Source |
|---------------|------------------|
| `claude*` | `Xenova/claude-tokenizer` |
| `gpt-4*`, `gpt-3.5*` | `Xenova/gpt-4` |
| `llama*`, `meta-llama/*` | `meta-llama/Llama-3.2-1B` |
| `mistral*` | `mistralai/Mistral-7B-v0.1` |

Users can override or add mappings via builder:

```rust
Ratatoskr::builder()
    .openrouter(key)
    .tokenizer_mapping("my-custom-model", TokenizerSource::Local {
        path: "/models/custom-tokenizer.json".into()
    })
    .build()
```

---

## 4. FastEmbed Provider (Local Embeddings)

```rust
// src/providers/fastembed.rs

pub struct FastEmbedProvider {
    model: fastembed::TextEmbedding,
    model_info: EmbeddingModelInfo,
}

pub struct EmbeddingModelInfo {
    pub name: String,
    pub dimensions: usize,
}

/// Supported embedding models (maps to fastembed's enum)
pub enum LocalEmbeddingModel {
    AllMiniLmL6V2,       // 384 dims, fast, good quality
    AllMiniLmL12V2,      // 384 dims, slightly better
    BgeSmallEn,          // 384 dims, strong retrieval
    BgeBaseEn,           // 768 dims, higher quality
    // ... expose what fastembed supports
}

impl FastEmbedProvider {
    pub fn new(model: LocalEmbeddingModel) -> Result<Self> {
        let options = fastembed::InitOptions::new(model.into())
            .with_show_download_progress(true)
            .with_cache_dir(cache_dir()?);

        let model = fastembed::TextEmbedding::try_new(options)?;
        Ok(Self { model, model_info: model.into() })
    }

    pub fn embed(&self, text: &str) -> Result<Embedding> {
        let vectors = self.model.embed(vec![text], None)?;
        Ok(Embedding {
            values: vectors.into_iter().next().unwrap(),
            model: self.model_info.name.clone(),
            dimensions: self.model_info.dimensions,
        })
    }

    pub fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Embedding>> {
        let vectors = self.model.embed(texts.to_vec(), None)?;
        // ... map to Embedding structs
    }
}
```

Note: `fastembed` is synchronous â€” wrap calls in `spawn_blocking` for async context.

---

## 5. ONNX NLI Provider (Local Cross-Encoder)

```rust
// src/providers/onnx_nli.rs

pub struct OnnxNliProvider {
    session: ort::Session,
    tokenizer: tokenizers::Tokenizer,
    model_info: NliModelInfo,
    device: Device,
}

pub struct NliModelInfo {
    pub name: String,
    pub labels: Vec<String>,  // ["entailment", "neutral", "contradiction"]
}

pub enum Device {
    Cpu,
    Cuda { device_id: u32 },
}

/// Supported NLI models
pub enum LocalNliModel {
    /// cross-encoder/nli-deberta-v3-base â€” good balance of speed/accuracy
    NliDebertaV3Base,
    /// cross-encoder/nli-deberta-v3-small â€” faster, slightly less accurate
    NliDebertaV3Small,
    /// Custom model from path
    Custom { model_path: PathBuf, tokenizer_path: PathBuf },
}

impl OnnxNliProvider {
    pub fn new(model: LocalNliModel, device: Device) -> Result<Self> {
        let (model_path, tokenizer_path) = model.resolve_paths()?;

        let session = ort::Session::builder()?
            .with_execution_providers([execution_provider(&device)?])?
            .commit_from_file(&model_path)?;

        let tokenizer = tokenizers::Tokenizer::from_file(&tokenizer_path)?;

        Ok(Self { session, tokenizer, model_info: model.into(), device })
    }

    pub fn infer_nli(&self, premise: &str, hypothesis: &str) -> Result<NliResult> {
        let encoding = self.tokenizer.encode((premise, hypothesis), true)?;

        let input_ids = encoding.get_ids();
        let attention_mask = encoding.get_attention_mask();

        let outputs = self.session.run(ort::inputs![
            "input_ids" => input_ids,
            "attention_mask" => attention_mask,
        ]?)?;

        let logits = outputs["logits"].try_extract_tensor::<f32>()?;
        let probs = softmax(&logits);

        Ok(NliResult {
            entailment: probs[0],
            neutral: probs[1],
            contradiction: probs[2],
            label: NliLabel::from_max(&probs),
        })
    }

    pub fn infer_nli_batch(&self, pairs: &[(&str, &str)]) -> Result<Vec<NliResult>> {
        // Batch encode and run inference â€” more efficient than N calls
    }
}

fn execution_provider(device: &Device) -> Result<ort::ExecutionProvider> {
    match device {
        Device::Cpu => Ok(ort::CPUExecutionProvider::default().build()),
        Device::Cuda { device_id } => Ok(ort::CUDAExecutionProvider::default()
            .with_device_id(*device_id)
            .build()),
    }
}

fn softmax(logits: &[f32]) -> Vec<f32> {
    let max = logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    let exps: Vec<f32> = logits.iter().map(|x| (x - max).exp()).collect();
    let sum: f32 = exps.iter().sum();
    exps.iter().map(|x| x / sum).collect()
}
```

---

## 6. Model Management

```rust
// src/model/manager.rs

pub struct ModelManager {
    embedding_models: RwLock<HashMap<String, Arc<FastEmbedProvider>>>,
    nli_models: RwLock<HashMap<String, Arc<OnnxNliProvider>>>,
    config: ModelManagerConfig,
}

pub struct ModelManagerConfig {
    pub cache_dir: PathBuf,           // ~/.cache/ratatoskr/models/
    pub default_device: Device,
    // Future Phase 5 additions:
    // pub memory_budget_bytes: Option<u64>,
    // pub idle_timeout: Option<Duration>,
}

pub enum ModelSource {
    HuggingFace { repo_id: String },
    Local { path: PathBuf },
}

impl ModelManager {
    pub fn new(config: ModelManagerConfig) -> Self;

    /// Get or lazily load an embedding model
    pub fn embedding(&self, model: LocalEmbeddingModel) -> Result<Arc<FastEmbedProvider>> {
        let key = model.cache_key();

        // Fast path: already loaded
        if let Some(provider) = self.embedding_models.read().get(&key) {
            return Ok(Arc::clone(provider));
        }

        // Slow path: load and cache
        let mut models = self.embedding_models.write();
        // Double-check after acquiring write lock
        if let Some(provider) = models.get(&key) {
            return Ok(Arc::clone(provider));
        }

        let provider = Arc::new(FastEmbedProvider::new(model)?);
        models.insert(key, Arc::clone(&provider));
        Ok(provider)
    }

    /// Get or lazily load an NLI model
    pub fn nli(&self, model: LocalNliModel) -> Result<Arc<OnnxNliProvider>>;

    /// Explicitly preload models (for latency-sensitive deployments)
    pub fn preload_embedding(&self, model: LocalEmbeddingModel) -> Result<()>;
    pub fn preload_nli(&self, model: LocalNliModel) -> Result<()>;

    /// Unload a model (for future memory management)
    pub fn unload_embedding(&self, model: &str) -> bool;
    pub fn unload_nli(&self, model: &str) -> bool;

    /// List currently loaded models
    pub fn loaded_models(&self) -> LoadedModels;
}

pub struct LoadedModels {
    pub embeddings: Vec<String>,
    pub nli: Vec<String>,
}
```

Double-checked locking ensures thread-safe lazy loading.

---

## 7. Trait Additions

```rust
// src/traits.rs additions

#[async_trait]
pub trait ModelGateway: Send + Sync {
    // ===== Existing methods =====

    async fn chat_stream(...) -> Result<...>;
    async fn chat(...) -> Result<ChatResponse>;
    fn capabilities(&self) -> Capabilities;
    async fn embed(&self, text: &str, model: &str) -> Result<Embedding>;
    async fn embed_batch(&self, texts: &[&str], model: &str) -> Result<Vec<Embedding>>;
    async fn infer_nli(&self, premise: &str, hypothesis: &str, model: &str) -> Result<NliResult>;
    async fn classify_zero_shot(&self, text: &str, labels: &[&str], model: &str) -> Result<ClassifyResult>;
    fn count_tokens(&self, text: &str, model: &str) -> Result<usize>;

    // ===== New methods =====

    /// Batch NLI inference â€” more efficient for multiple pairs
    async fn infer_nli_batch(
        &self,
        pairs: &[(&str, &str)],
        model: &str,
    ) -> Result<Vec<NliResult>> {
        // Default: sequential fallback
        let mut results = Vec::with_capacity(pairs.len());
        for (premise, hypothesis) in pairs {
            results.push(self.infer_nli(premise, hypothesis, model).await?);
        }
        Ok(results)
    }

    /// Non-streaming text generation
    async fn generate(
        &self,
        _prompt: &str,
        _options: &GenerateOptions,
    ) -> Result<GenerateResponse> {
        Err(RatatoskrError::NotImplemented("generate"))
    }

    /// Streaming text generation
    async fn generate_stream(
        &self,
        _prompt: &str,
        _options: &GenerateOptions,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<GenerateEvent>> + Send>>> {
        Err(RatatoskrError::NotImplemented("generate_stream"))
    }
}
```

```rust
// src/types/generate.rs (new file)

pub struct GenerateOptions {
    pub model: String,
    pub max_tokens: Option<usize>,
    pub temperature: Option<f32>,
    pub top_p: Option<f32>,
    pub stop_sequences: Vec<String>,
}

pub struct GenerateResponse {
    pub text: String,
    pub usage: Option<Usage>,
    pub model: Option<String>,
    pub finish_reason: FinishReason,
}

pub enum GenerateEvent {
    Text(String),
    Done,
}
```

---

## 8. Updated Builder & Routing

```rust
// src/gateway/builder.rs additions

impl RatatoskrBuilder {
    // ===== Existing =====
    pub fn openrouter(mut self, api_key: impl Into<String>) -> Self;
    pub fn anthropic(mut self, api_key: impl Into<String>) -> Self;
    pub fn huggingface(mut self, api_key: impl Into<String>) -> Self;

    // ===== New: Local inference =====

    /// Enable local embeddings via fastembed
    #[cfg(feature = "local-inference")]
    pub fn local_embeddings(mut self, model: LocalEmbeddingModel) -> Self;

    /// Enable local NLI via ONNX
    #[cfg(feature = "local-inference")]
    pub fn local_nli(mut self, model: LocalNliModel) -> Self;

    /// Set device for local inference (default: CPU)
    #[cfg(feature = "local-inference")]
    pub fn device(mut self, device: Device) -> Self;

    /// Custom tokenizer mapping
    pub fn tokenizer_mapping(
        mut self,
        model_pattern: impl Into<String>,
        source: TokenizerSource,
    ) -> Self;

    /// Set model cache directory (default: ~/.cache/ratatoskr/models/)
    #[cfg(feature = "local-inference")]
    pub fn cache_dir(mut self, path: impl Into<PathBuf>) -> Self;
}
```

```rust
// src/gateway/routing.rs additions

pub enum EmbedProvider {
    HuggingFace,
    #[cfg(feature = "local-inference")]
    FastEmbed,
}

pub enum NliProvider {
    HuggingFace,
    #[cfg(feature = "local-inference")]
    Onnx,
}
```

Usage:

```rust
// API-only (lightweight)
let gateway = Ratatoskr::builder()
    .openrouter(api_key)
    .huggingface(hf_key)
    .build()?;

// With local inference
let gateway = Ratatoskr::builder()
    .openrouter(api_key)
    .local_embeddings(LocalEmbeddingModel::AllMiniLmL6V2)
    .local_nli(LocalNliModel::NliDebertaV3Base)
    .device(Device::Cuda { device_id: 0 })
    .build()?;
```

---

## 9. Updated Capabilities

```rust
// src/types/capabilities.rs additions

pub struct Capabilities {
    // Existing
    pub chat: bool,
    pub streaming: bool,
    pub tool_use: bool,
    pub embeddings: bool,
    pub nli: bool,
    pub classification: bool,

    // New
    pub generate: bool,
    pub token_counting: bool,
    pub local_inference: bool,
}

impl Capabilities {
    /// Local inference capabilities (embeddings + NLI, no API needed)
    pub fn local_only() -> Self {
        Self {
            embeddings: true,
            nli: true,
            token_counting: true,
            local_inference: true,
            ..Default::default()
        }
    }

    /// Full capabilities (chat + local + API)
    pub fn full() -> Self {
        Self {
            chat: true,
            streaming: true,
            tool_use: true,
            embeddings: true,
            nli: true,
            classification: true,
            generate: true,
            token_counting: true,
            local_inference: true,
        }
    }
}
```

---

## 10. Project Structure

```
src/
â”œâ”€â”€ lib.rs                      # Public API re-exports
â”œâ”€â”€ error.rs                    # RatatoskrError, Result
â”œâ”€â”€ traits.rs                   # ModelGateway trait (updated)
â”‚
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ message.rs
â”‚   â”œâ”€â”€ options.rs
â”‚   â”œâ”€â”€ response.rs
â”‚   â”œâ”€â”€ tool.rs
â”‚   â”œâ”€â”€ capabilities.rs         # Updated with new flags
â”‚   â”œâ”€â”€ future.rs               # Embedding, NliResult, ClassifyResult
â”‚   â””â”€â”€ generate.rs             # NEW: GenerateOptions, GenerateResponse
â”‚
â”œâ”€â”€ gateway/
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ embedded.rs             # Updated to use new providers
â”‚   â”œâ”€â”€ builder.rs              # Updated with local inference config
â”‚   â””â”€â”€ routing.rs              # Updated with new provider variants
â”‚
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ huggingface.rs          # Existing API provider
â”‚   â”œâ”€â”€ fastembed.rs            # NEW: local embeddings
â”‚   â””â”€â”€ onnx_nli.rs             # NEW: local NLI
â”‚
â”œâ”€â”€ tokenizer/                   # NEW module
â”‚   â”œâ”€â”€ mod.rs                  # TokenizerProvider trait, registry
â”‚   â””â”€â”€ hf.rs                   # HuggingFace tokenizers impl
â”‚
â”œâ”€â”€ model/                       # NEW module
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ manager.rs              # ModelManager, lazy loading
â”‚   â”œâ”€â”€ source.rs               # ModelSource, download logic
â”‚   â””â”€â”€ device.rs               # Device enum, execution providers
â”‚
â””â”€â”€ convert/
    â””â”€â”€ mod.rs                  # ratatoskr â†” llm conversions

tests/
â”œâ”€â”€ # existing tests...
â”œâ”€â”€ tokenizer_test.rs           # NEW
â”œâ”€â”€ fastembed_test.rs           # NEW
â”œâ”€â”€ onnx_nli_test.rs            # NEW
â””â”€â”€ local_inference_live_test.rs # NEW (ignored, manual)
```

---

## Implementation Tasks

### âœ… Task 1: Add Dependencies (COMPLETED)
- âœ… Update `Cargo.toml` with new features and dependencies
- âœ… Added: fastembed v5, ort v2.0.0-rc, tokenizers v0.22, hf-hub v0.4, dirs v5
- âœ… Features: local-inference, cuda
- âœ… Verify: `cargo check --features local-inference`

### âœ… Task 2: Token Counting Module (COMPLETED)
- âœ… Create `src/tokenizer/mod.rs` with `TokenizerProvider` trait
- âœ… Create `src/tokenizer/hf.rs` with `HfTokenizer` implementation
- âœ… Create `TokenizerRegistry` with default mappings (claude, gpt-4, llama, mistral)
- âœ… Lazy loading with double-checked locking
- âœ… Alias support with recursive resolution
- âœ… Create `tests/tokenizer_test.rs` (5 tests passing)
- âœ… Added Configuration and DataError to RatatoskrError
- âœ… Verify: `cargo test --test tokenizer_test --features local-inference`

### âœ… Task 3: Generate Types (COMPLETED)
- âœ… Create `src/types/generate.rs` with `GenerateOptions`, `GenerateResponse`, `GenerateEvent`
- âœ… Builder pattern for GenerateOptions
- âœ… Update `src/types/mod.rs` exports
- âœ… Update `src/lib.rs` exports
- âœ… Verify: `cargo check`

### âœ… Task 4: Trait Additions (COMPLETED)
- âœ… Add `infer_nli_batch()` with default impl to `ModelGateway`
- âœ… Add `generate()` and `generate_stream()` stubs to `ModelGateway`
- âœ… Update `tests/traits_test.rs` (5 tests passing)
- âœ… Verify: `cargo test --test traits_test`

### âœ… Task 5: Device & Model Source Types (COMPLETED)
- âœ… Create `src/model/mod.rs`
- âœ… Create `src/model/device.rs` with `Device` enum (Cpu, Cuda)
- âœ… Create `src/model/source.rs` with `ModelSource` enum and download logic
- âœ… Create `src/model/manager.rs` stub (ModelManager, ModelManagerConfig, LoadedModels)
- âœ… Note: execution_provider helper deferred to Task 7
- âœ… Verify: `cargo check --features local-inference`

### âœ… Task 6: FastEmbed Provider (COMPLETED)
- âœ… Create `src/providers/fastembed.rs`
- âœ… Add `LocalEmbeddingModel` enum (4 variants: AllMiniLmL6V2, AllMiniLmL12V2, BgeSmallEn, BgeBaseEn)
- âœ… Implement `FastEmbedProvider` with embed/embed_batch (requires &mut self)
- âœ… Model properties: name(), dimensions(), cache_key()
- âœ… EmbeddingModelInfo struct
- âœ… Create `tests/fastembed_test.rs` (1 test passing)
- âœ… Verify: `cargo test --test fastembed_test --features local-inference`

**Commit:** `0a17cff` - feat(local-inference): add phase 3 foundation (tasks 1-6)

### âœ… Task 7: ONNX NLI Provider (COMPLETED)
- âœ… Create `src/providers/onnx_nli.rs`
- âœ… Add `LocalNliModel` enum (NliDebertaV3Base, NliDebertaV3Small, Custom)
- âœ… Implement `OnnxNliProvider` with infer_nli/infer_nli_batch
- âœ… HuggingFace Hub integration for model downloads
- âœ… Create `tests/onnx_nli_test.rs` (3 unit tests + 2 ignored live tests)
- âœ… Verify: `cargo test --test onnx_nli_test --features local-inference`

### âœ… Task 8: Model Manager (COMPLETED)
- âœ… Create `src/model/manager.rs`
- âœ… Implement lazy loading with double-checked locking
- âœ… Add preload/unload methods
- âœ… Thread-safe Arc<RwLock<>> wrapping for providers
- âœ… Create `tests/model_manager_test.rs` (5 unit tests + 2 ignored live tests)
- âœ… Verify: `cargo test --test model_manager_test --features local-inference`

### âœ… Task 9: Update Capabilities (COMPLETED)
- âœ… Add new fields to `Capabilities` struct (generate, tool_use, local_inference)
- âœ… Add `local_only()` constructor
- âœ… Update `full()` constructor
- âœ… Update merge() to include new fields
- âœ… Update `capabilities_test.rs` (now 9 tests)
- âœ… Verify: `cargo test --test capabilities_test`

### âœ… Task 10: Update Builder (COMPLETED)
- âœ… Add `local_embeddings()`, `local_nli()`, `device()`, `cache_dir()` methods
- âœ… Add `tokenizer_mapping()` method
- âœ… Update routing enums (EmbedProvider::FastEmbed, NliProvider::Onnx)
- âœ… Update `gateway_test.rs` (now 6 tests)
- âœ… Verify: `cargo test --test gateway_test --features local-inference`

### âœ… Task 11: Update EmbeddedGateway (COMPLETED)
- âœ… Integrate `ModelManager` and `TokenizerRegistry` into struct
- âœ… Implement `count_tokens()` using registry
- âœ… Update `capabilities()` to reflect local inference
- âœ… Builder constructs router with local providers and passes to gateway
- â¸ï¸ Actual routing of embed/NLI calls to local providers (deferred â€” infrastructure ready)
- âœ… Verify: `cargo test --features local-inference`

### Task 12: Implement generate() via llm crate
- Add `generate()` and `generate_stream()` implementations
- Create `tests/generate_test.rs`
- Verify: `cargo test --test generate_test`

### Task 13: Live Tests
- Create `tests/local_inference_live_test.rs` with `#[ignore]` tests
- Test local embeddings with real model
- Test local NLI with real model
- Test token counting
- Verify: `cargo test --test local_inference_live_test --features local-inference -- --ignored`

### Task 14: Full Test Suite & Lint
- Verify: `just pre-push`

### Task 15: Update Documentation
- Update `CLAUDE.md` with Phase 3-4 status
- Update `src/lib.rs` doc examples
- Update architecture appendix
- Verify: `cargo doc --no-deps --features local-inference`
